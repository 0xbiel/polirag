\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{longtable}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{tgheros}
\usepackage{fancyhdr}

\geometry{margin=1in}

% Sans-serif for sections
\titleformat{\section}{\large\sffamily\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\sffamily\bfseries}{\thesubsection}{1em}{}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.98}
\definecolor{primaryblue}{rgb}{0.12, 0.29, 0.49}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    rulecolor=\color{codegray!30}
}
\lstset{style=mystyle}

\hypersetup{
    colorlinks=true,
    linkcolor=primaryblue,
    filecolor=magenta,      
    urlcolor=primaryblue,
}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{1cm}
    {\huge\bfseries\sffamily PoliRag\par}
    \vspace{0.5cm}
    {\large PoliformaT Retrieval-Augmented Generation\par}
    \vspace{2cm}
    {\Large\itshape 0xbiel\par}
    \vfill
    {\large \today\par}
\end{titlepage}
\newpage
\tableofcontents
\newpage

\section{Introduction}
PoliRag (PoliformaT Retrieval-Augmented Generation) is a comprehensive personal knowledge base application designed for students of the Universitat Politècnica de València (UPV). It bridges the gap between the university's Learning Management System (PoliformaT) and modern Large Language Models (LLMs). By automating the ingestion of course materials and providing a local, semantic search interface, PoliRag allows users to interact with their curriculum in natural language.

The design philosophy emphasizes \textbf{local-first privacy}, \textbf{efficiency}, and \textbf{robustness}. No user data leaves the machine during the retrieval phase, and the system is designed to handle network instability and varied content formats.

\section{System Architecture}
The system is built in Rust, leveraging its memory safety and concurrency features. It follows a modular architecture where the core domain logic (RAG) is decoupled from the user interface (TUI) and data acquisition (Scraper).

\subsection{High-Level Component Diagram}
\begin{itemize}
    \item \textbf{User Interface Layer}: Textual User Interface (TUI) built with \texttt{ratatui}.
    \item \textbf{Application Layer}: Orchestrates the sync pipeline, query handling, and state management.
    \item \textbf{Domain Layer}:
        \begin{itemize}
            \item \textbf{RAG Service}: Embeddings, Vector Search, Document Management.
            \item \textbf{Scraper Service}: UPV PoliformaT Automation.
        \end{itemize}
    \item \textbf{Infrastructure Layer}:
        \begin{itemize}
            \item \textbf{HNSW Index}: On-disk vector storage.
            \item \textbf{LLM Client}: HTTP client for OpenRouter/LM Studio.
            \item \textbf{Config Store}: JSON based persistence with encryption.
        \end{itemize}
\end{itemize}

\section{Detailed Component Analysis}

\subsection{Scraper Module (\texttt{src/scrapper})}
The scraper automation is powered by \texttt{headless\_chrome}, which controls a local Chrome instance via the DevTools Protocol (CDP).

\subsubsection{Authentication \& Session Management}
PoliformaT requires UPV Single Sign-On (SSO). PoliRag creates a robust session flow:
\begin{enumerate}
    \item \textbf{Credential Resolution}: Checks (1) Cached Config, (2) Environment Variables (\texttt{POLIFORMAT\_USER}).
    \item \textbf{Headless Login}: 
        \begin{itemize}
            \item Navigates to the login portal.
            \item Detects input fields dynamically (handling variable IDs like \texttt{\#username} vs \texttt{input[name='dni']}).
            \item Submits credentials and waits for the \texttt{\#toolMenu} element to confirm success.
        \end{itemize}
    \item \textbf{Cookie Export}: Upon successful login, the scraper exports the browser cookies. These are shared with a \texttt{reqwest::Client} instance, enabling high-performance concurrent HTTP requests for subsequent operations that don't require JavaScript rendering.
\end{enumerate}

\subsubsection{Parallel Execution Strategy}
While navigating subjects is sequential (to manage the browser's download behavior context safely), content extraction within a subject utilizes hybrid blocking/async tasks.
\begin{itemize}
    \item \textbf{Subject Discovery}: Executes a JS snippet in the browser context to parse the DOM and extract all subject URLs, deduplicating them.
    \item \textbf{Download Management}: The scraper uses \texttt{Page.setDownloadBehavior} to redirect downloads to a per-subject directory. It monitors file system events (presence of \texttt{.crdownload} files) to ensure completion.
    \item \textbf{Enhanced Content Extraction}: Beyond PDF files, the scraper now automatically navigates to the "Guia Docent" and "Syllabus" pages to extract rich text descriptions and professor information, providing a more comprehensive context for the RAG system.
\end{itemize}

\subsection{RAG Engine (\texttt{src/rag})}
The core of PoliRag is its vector search capabilities.

\subsubsection{Embedding Model}
PoliRag utilizes a dynamic embedding system powered by a custom \texttt{embed\_model!} macro.
\begin{itemize}
    \item \textbf{Dynamic Loading}: The macro allows for switching between different models (e.g., \texttt{google/embedding-gemma-300m} or \texttt{nomic-embed-text}) based on configuration, without recompiling the entire logic.
    \item \textbf{Model}: Defaults to quantized GGUF models (e.g., \texttt{Q4\_0} precision).
    \item \textbf{Engine}: \texttt{llama.cpp} bindings.
    \item \textbf{Hardware Acceleration}: On macOS, the system offloads all layers to Metal (GPU) for near-instant inference.
    \item \textbf{Chunking Strategy}: Uses overlapping semantic chunks (max 512 tokens) to preserve contextual coherence across document fragments.
\end{itemize}

\subsubsection{Vector Storage (HNSW)}
Instead of a linear scan, PoliRag uses Hierarchical Navigable Small World (HNSW) graphs via the \texttt{hnsw\_rs} crate.
\begin{itemize}
    \item \textbf{Algorithm}: Constructs a multi-layer graph where higher layers act as expressways for search, descending to lower layers for precision.
    \item \textbf{Parameters}: M=24 (max links per node), ef\_construction=10000 (candidates during build).
    \item \textbf{Persistence}: The index is serialized to two files:
        \begin{itemize}
            \item \texttt{.hnsw.graph}: The graph structure.
            \item \texttt{.data}: The document content and metadata (managed via \texttt{bincode}).
        \end{itemize}
\end{itemize}

\subsection{TUI \& Event Loop (\texttt{src/tui})}
The interface follows a reactor pattern.

\subsubsection{Async Architecture}
The main thread runs the UI render loop. Heavy operations (LLM generation, Sync) are offloaded to \texttt{tokio} tasks which communicate results back via \texttt{mpsc} channels.
\begin{itemize}
    \item \texttt{tx\_llm / rx\_llm}: Stream tokens from the LLM.
    \item \texttt{tx\_sync / rx\_sync}: Stream log messages during the scraping process.
\end{itemize}

\subsubsection{Rendering Pipeline}
\begin{itemize}
    \item \textbf{Markdown Caching}: Re-parsing markdown every frame is CPU-intensive. PoliRag implements a cell-level cache for rendered lines, enabling fluid 60FPS scrolling and near real-time text streaming even with complex formatting.
    \item \textbf{Throbber \& Progress Status}: Animated indicators provide feedback for background processes like scraping or "thinking" states.
    \item \textbf{Thinking Blocks}: Native support for DeepSeek-style reasoning models. The UI detects \texttt{<think>} tags and allows users to toggle the visibility of the chain-of-thought, keeping the interface clean.
    \item \textbf{Intelligent Filename Detection}: The chat interface matches keywords against the local document index. If a user asks for a specific file (e.g., "summarize 'p1.pdf'"), the system retrieves the full content rather than just snippets.
\end{itemize}

\section{Data Structures}

\subsection{Document}
The foundational unit of the knowledge base.
\begin{lstlisting}[language=Rust]
#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct Document {
    pub id: String,                 // Unique URI (e.g., subject_id/file_path)
    pub content: String,            // The raw text content
    pub embedding: Vec<f32>,        // High-dimensional vector
    pub metadata: HashMap<String, String>, // Key-values: type, user_id, timestamp
    pub user_id: String,            // Multi-user support field
}
\end{lstlisting}

\subsection{Configuration}
Stored in standard OS locations (e.g., \texttt{\textasciitilde/Library/Application Support/polirag/config.json} on macOS).
\begin{lstlisting}[language=Rust]
pub struct Config {
    pub last_model: Option<String>,
    pub cached_credentials: Option<EncryptedCredentials>, // Simple XOR + Base64
    pub llm_provider: LlmProvider, // LmStudio | OpenRouter
    // ...
}
\end{lstlisting}

\section{Error Handling \& Security}
\begin{itemize}
    \item \textbf{Credential Security}: Credentials are not stored in plain text. A simple XOR encryption with a hardcoded key obfuscates them on disk, preventing casual snooping (though not robust against determined attackers with binary access).
    \item \textbf{Graceful Degradation}: If the HNSW index is corrupted or missing, the system attempts to rebuild it or start fresh rather than crashing.
    \item \textbf{Network Resilience}: The scraper includes retry logic for finding DOM elements, handling the slow loading times of the university portal.
\end{itemize}

\end{document}
